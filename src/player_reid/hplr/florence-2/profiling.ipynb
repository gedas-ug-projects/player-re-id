{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import torch\n",
    "import logging\n",
    "import random\n",
    "import warnings\n",
    "import argparse\n",
    "import cProfile\n",
    "import pstats\n",
    "import torch.multiprocessing as mp\n",
    "import time\n",
    "from typing import Optional, List\n",
    "\n",
    "from argparse import Namespace\n",
    "from glob import glob\n",
    "from PIL import Image, ImageOps\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoProcessor, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "from optimum.bettertransformer import BetterTransformer\n",
    "from optimum.onnxruntime import ORTModelForCausalLM\n",
    "from optimum.onnxruntime.configuration import AutoQuantizationConfig\n",
    "from optimum.onnxruntime import ORTOptimizer\n",
    "\n",
    "warnings.simplefilter(\"ignore\", FutureWarning)\n",
    "logging.basicConfig(\n",
    "    level=logging.DEBUG, format=\"%(asctime)s - %(levelname)s - %(message)s\"\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "TOTAL_GPUS = 8\n",
    "BOOTSTRAPS = 3\n",
    "\n",
    "# TODO: Optimize prompt length?\n",
    "PROMPT = \"\"\"Identify the jersey number of the basketball player in the frame. If none, return None. Output only the digits:\n",
    "<jersey_number>\n",
    "[EOS]\"\"\"\n",
    "\n",
    "# TODO:\n",
    "# 1. Optimum -- NO SUPPORT\n",
    "# 2. to_bettertransformer() -- NO SUPPORT\n",
    "# 3. torch.backends.cuda.sdp_kernel\n",
    "# 4. tensorRT (no quantization): https://huggingface.co/docs/optimum/onnxruntime/usage_guides/gpu#accelerated-inference-on-nvidia-gpus\n",
    "# 5. autocast -- NO SPEEDUP vs. HALF()\n",
    "# 6. 4/8bit quantization -- NO SUPPORT\n",
    "# 7. # bootstraps 9 -> 3\n",
    "# 8. florence large -> florence-base\n",
    "# 9. export for onnx + onnx rt\n",
    "# 10. torch dataloader?\n",
    "# 11. paralellize data pre-processing w/ processor obj.\n",
    "\n",
    "def load_model_and_tokenizer(device: int = 0, args=None):\n",
    "    # compile_model = args.compile_model\n",
    "    # precision = args.precision\n",
    "    try:\n",
    "        logger.info(\"Loading model and tokenizer...\")\n",
    "        model = (\n",
    "            AutoModelForCausalLM.from_pretrained(\n",
    "                # \"microsoft/Florence-2-large-ft\",\n",
    "                \"microsoft/Florence-2-base-ft\",\n",
    "                trust_remote_code=True,\n",
    "                device_map=\"cuda\",\n",
    "            )\n",
    "            .eval()\n",
    "            .to(device)\n",
    "        )\n",
    "\n",
    "        # attempt to speed up inference by compiling model JIT\n",
    "        # if compile_model == \"True\":\n",
    "        #     logger.info(\"Compiling model...\")\n",
    "        #     model = torch.compile(model, mode=\"max-autotune\")\n",
    "        \n",
    "        # optimizer = ORTOptimizer.from_pretrained(model)\n",
    "        # quant_config = AutoQuantizationConfig.avx512_vnni()\n",
    "        # optimizer.optimize_model(\"onnx_model.onnx\", quantization_config=quant_config)\n",
    "\n",
    "        model = torch.compile(model)\n",
    "        processor = AutoProcessor.from_pretrained(\n",
    "            # \"microsoft/Florence-2-large-ft\", \n",
    "            \"microsoft/Florence-2-base-ft\", \n",
    "            trust_remote_code=True,\n",
    "        )\n",
    "        return model, processor\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to load model or tokenizer: {e}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, processor = load_model_and_tokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use half precision\n",
    "half = model.half()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_valid_jersey_number(text):\n",
    "    # TODO: what about the number \"00\"?\n",
    "    if text.isdigit():\n",
    "        number = int(text)\n",
    "        return 0 <= number <= 99\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BitsAndBytesConfig\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from torch.cuda.amp import autocast\n",
    "\n",
    "\n",
    "BOOTSTRAPS = 1\n",
    "\n",
    "\n",
    "def preprocess_image(image, prompt, device):\n",
    "    inputs = processor(text=prompt, images=image, return_tensors=\"pt\")\n",
    "    input_ids = inputs[\"input_ids\"].to(device, non_blocking=True)\n",
    "    pixel_values = inputs[\"pixel_values\"].to(device, non_blocking=True).half()\n",
    "    return input_ids, pixel_values\n",
    "\n",
    "def ocr(\n",
    "    image_file_paths: List[str],\n",
    "    model,\n",
    "    processor,\n",
    "    device: int = 0,\n",
    "    args: Optional[dict] = None,\n",
    ") -> Optional[List[str]]:\n",
    "    logger = logging.getLogger(__name__)\n",
    "    logger.setLevel(logging.DEBUG)\n",
    "    bootstraped_results = []\n",
    "\n",
    "    def load_image(fp):\n",
    "        try:\n",
    "            image = Image.open(fp)\n",
    "            image.load()\n",
    "            return image\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed to load image {fp}: {e}\")\n",
    "            return None\n",
    "\n",
    "    # Load images in parallel\n",
    "    start = time.time()\n",
    "    with ThreadPoolExecutor() as executor:\n",
    "        images = list(executor.map(load_image, image_file_paths))\n",
    "    images = [img for img in images if img is not None]\n",
    "    end = time.time()\n",
    "    logger.debug(f\"Images loaded in: {end - start:.2f}s\")\n",
    "\n",
    "    if not images:\n",
    "        logger.error(\"No valid images loaded.\")\n",
    "        return None\n",
    "\n",
    "    # TODO: maybe resize images before using processor\n",
    "    prompts = [PROMPT] * len(images)\n",
    "    \n",
    "    start = time.time()\n",
    "    inputs = processor(text=prompts, images=images, return_tensors=\"pt\")\n",
    "    end = time.time()\n",
    "    logger.debug(f\"Preprocessing inputs took: {end - start:.2f}s\")\n",
    "\n",
    "    start = time.time()\n",
    "    input_ids = inputs[\"input_ids\"].to(device, non_blocking=True)\n",
    "    pixel_values = inputs[\"pixel_values\"].to(device, non_blocking=True).half()\n",
    "    end = time.time()\n",
    "    del inputs\n",
    "    logger.debug(f\"Copying + deleting inputs took: {end - start:.2f}s\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "        start = time.time()\n",
    "        generated_ids = model.generate(\n",
    "            input_ids=input_ids,\n",
    "            pixel_values=pixel_values,\n",
    "            max_new_tokens=5,\n",
    "            do_sample=False,\n",
    "            early_stopping=False,\n",
    "            num_beams=BOOTSTRAPS,\n",
    "            num_return_sequences=BOOTSTRAPS,\n",
    "        )\n",
    "        end = time.time()\n",
    "        logger.debug(f\"Generating ids took: {end - start:.2f}s\")\n",
    "\n",
    "    # decode the generated text\n",
    "    start = time.time()\n",
    "    generated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)\n",
    "    end = time.time()\n",
    "    logger.debug(f\"Batch decoding took: {end - start:.2f}s\")\n",
    "\n",
    "    # post-process the output\n",
    "    start = time.time()\n",
    "    for gt, image in zip(generated_text, images):\n",
    "        parsed_answer = processor.post_process_generation(\n",
    "            gt, task=\"<OCR>\", image_size=(image.width, image.height)\n",
    "        )\n",
    "        bootstraped_results.append(parsed_answer)\n",
    "    end = time.time()\n",
    "    logger.debug(f\"Post processing outputs took: {end - start:.2f}s\")\n",
    "\n",
    "    # except Exception as e:\n",
    "    #     logger.error(f\"OCR processing failed: {e}\")\n",
    "    #     return None\n",
    "\n",
    "    return bootstraped_results if bootstraped_results else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ex_img_6 = \"/mnt/opr/levlevi/player-re-id/src/testing/constrastive_matching/clip_reid/data/data_reid/reid_challenge/gallery/00955.jpeg\"\n",
    "batch_size = 96\n",
    "\n",
    "start = time.time()\n",
    "results = ocr([ex_img_6] * batch_size, half, processor)\n",
    "end = time.time()\n",
    "print(f\"Total inference time: {end-start}s\")\n",
    "print(f\"Inference time per prompt: {(end-start) / batch_size}s\")\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Batch Size | Time/Image (Sec.) |\n",
    "| :---: | :---: |\n",
    "| 1 | 0.5399 | \n",
    "| 2 | 0.3558 |\n",
    "| 4 | 0.2159 |\n",
    "| **6** | **0.2114** |\n",
    "| 8 | OOM |\n",
    "\n",
    "--- \n",
    "\n",
    "| Batch Size | `.half()` | `autocast()` | Time/Image (Sec.) |\n",
    "| :---: | :---: |  :---: | :---: |\n",
    "| 4 | No | No | 0.2159 |\n",
    "| 4 | Yes | No | 0.0949 |\n",
    "| 8 | No | No | OOM |\n",
    "| 8 | Yes | No | 0.0856 |\n",
    "| 16 | Yes | No | 0.0808 |\n",
    "| 32 | Yes | No | 0.0785 |\n",
    "| 64 | Yes | No | 0.0783 |\n",
    "| 64 | Yes | Yes | 0.1006 |\n",
    "| **96** | **Yes** | No | **0.0778** |\n",
    "| 128 | Yes | No | OOM |\n",
    "\n",
    "---\n",
    "\n",
    "| `torch.backends.cuda.matmul.allow_tf32` | `torch.backends.cudnn.benchmark` | Time/Image (Sec.) |\n",
    "| :---: | :---: | :---: |\n",
    "| No | No | 0.0902 |\n",
    "| Yes | No | 0.0922 |\n",
    "| No | Yes | 0.0962 |\n",
    "| Yes | Yes | 0.0960 |\n",
    "\n",
    "---\n",
    "\n",
    "| Tokenizer | Time/Image (Sec.) |\n",
    "| :---: | :---: |\n",
    "| Default | **0.06187** | \n",
    "| `bart-base` | 0.06196 |\n",
    "\n",
    "---\n",
    "\n",
    "| Model Varient | Time/Image (Sec.) |\n",
    "| :---: | :---: |\n",
    "| `large-ft` | 0.06187 | \n",
    "| `base-ft` | **0.03936** |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "\n",
    "1 / 0.03650"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BartForConditionalGeneration, BartTokenizer, BartTokenizerFast\n",
    "\n",
    "\n",
    "# tokenizer = BartTokenizerFast.from_pretrained(\"microsoft/Florence-2-large-ft\")\n",
    "tok = BartTokenizerFast.from_pretrained(\"facebook/bart-base\")\n",
    "processor = AutoProcessor.from_pretrained(\n",
    "    \"microsoft/Florence-2-large-ft\",\n",
    "    trust_remote_code=True,\n",
    "    # tokenizer=tok,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
