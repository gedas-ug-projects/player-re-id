{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/playpen-storage/levlevi/anaconda3/envs/vish/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/playpen-storage/levlevi/anaconda3/envs/vish/lib/python3.8/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import warnings\n",
    "\n",
    "from PIL import Image\n",
    "from transformers import AutoProcessor, AutoModelForCausalLM\n",
    "warnings.simplefilter(\"ignore\", FutureWarning)\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\"microsoft/Florence-2-large-ft\", trust_remote_code=True, device_map=\"cuda\").eval()\n",
    "processor = AutoProcessor.from_pretrained(\"microsoft/Florence-2-large-ft\", trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **OCR Benchmark**\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "from PIL import Image\n",
    "from glob import glob\n",
    "import ast\n",
    "import sys\n",
    "\n",
    "THRESH = 0.00\n",
    "yolo_model = YOLO(\"/playpen-storage/levlevi/player-re-id/src/testing/ocr_model_comparisons/text_detection/runs/detect/train4/weights/best.pt\")\n",
    "\n",
    "def det_bbox(image: Image): \n",
    "    # idk if this will work\n",
    "    results = yolo_model([image], verbose=False)\n",
    "    for _, result in enumerate(results):\n",
    "        x1_min = sys.maxsize\n",
    "        x2_max = 0\n",
    "        y1_min = sys.maxsize\n",
    "        y2_max = 0\n",
    "        json_obj = ast.literal_eval(result.tojson())\n",
    "        if len(json_obj) == 0:\n",
    "            return None, None, None, None\n",
    "        for pred in json_obj:\n",
    "            if pred.get('confidence') < THRESH:\n",
    "                continue\n",
    "            x1, y1, x2, y2 = [pred.get('box').get(k) for k in pred.get('box')]\n",
    "            if x1 < x1_min:\n",
    "                x1_min = x1\n",
    "            if x2 > x2_max:\n",
    "                x2_max = x2\n",
    "            if y1 < y1_min:\n",
    "                y1_min = y1\n",
    "            if y2 > y2_max:\n",
    "                y2_max = y2\n",
    "        if x2_max == 0:\n",
    "            return None, None, None, None\n",
    "        else:\n",
    "            return int(x1_min), int(y1_min), int(x2_max), int(y2_max)\n",
    "    return None, None, None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:53<00:00,  1.86it/s]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from tqdm import tqdm\n",
    "from glob import glob\n",
    "\n",
    "test_set_dir = \"/playpen-storage/levlevi/player-re-id/src/testing/ocr_model_comparisons/text_recognition/nba_100_test_set\"\n",
    "image_file_paths = glob(os.path.join(test_set_dir, '*.jpg'))\n",
    "PROMPT = \"\"\"Analyze the basketball player shown in the provided still tracklet frame and describe the following details:\n",
    "1. Jersey Number: Identify the number on the player's jersey. If the player has no jersey, provide None.\n",
    "Based on the frame description, produce an output prediction in the following JSON format:\n",
    "{\n",
    "  \"jersey_number\": \"<predicted_jersey_number>\",\n",
    "}\n",
    "[EOS]\"\"\"\n",
    "# PROMPT=\"<OCR>\"\n",
    "\n",
    "def crop_image(image: Image):\n",
    "    x1, y1, x2, y2 = det_bbox(Image.open(image_file_path))\n",
    "    if x1 is None:\n",
    "        return None\n",
    "    cropped_img = image.crop((x1, y1, x2, y2))\n",
    "    cropped_img.save('cropped_img.png')\n",
    "    return cropped_img\n",
    "\n",
    "def perform_ocr(image_file_path: str) -> str:\n",
    "    image = Image.open(image_file_path)\n",
    "    # image = crop_image(image)\n",
    "    if not image:\n",
    "        return None\n",
    "    inputs = processor(text=PROMPT, images=image, return_tensors=\"pt\").to('cuda')\n",
    "    generated_ids = model.generate(\n",
    "        input_ids=inputs[\"input_ids\"],\n",
    "        pixel_values=inputs[\"pixel_values\"],\n",
    "        max_new_tokens=1024,\n",
    "        do_sample=True,\n",
    "        num_beams=10\n",
    "    )\n",
    "    generated_text = processor.batch_decode(generated_ids, skip_special_tokens=False)[0]\n",
    "    parsed_answer = processor.post_process_generation(generated_text, task=\"<OCR>\", image_size=(image.width, image.height))\n",
    "    return parsed_answer\n",
    "\n",
    "ground_truth_labels = []\n",
    "results = []\n",
    "for image_file_path in tqdm(image_file_paths):\n",
    "    # get human annotation\n",
    "    ground_truth_label = image_file_path.split('/')[-1].split('_')[1].split('.')[0]\n",
    "    ground_truth_labels.append(ground_truth_label)\n",
    "    # perform ocr\n",
    "    result = perform_ocr(image_file_path)\n",
    "    results.append(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ground_truth_label</th>\n",
       "      <th>prediction</th>\n",
       "      <th>correct</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>na</td>\n",
       "      <td>nan</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>na</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>na</td>\n",
       "      <td>nan</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>36</td>\n",
       "      <td>nan</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  ground_truth_label prediction  correct\n",
       "0                 na        nan    False\n",
       "1                 na          1    False\n",
       "2                 na        nan    False\n",
       "3                  2          2     True\n",
       "4                 36        nan    False"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "ground_truth_labels = [f.split('/')[-1].split('_')[1].split('.')[0] for f in image_file_paths]\n",
    "predictions = []\n",
    "for r in results:\n",
    "    if not r:\n",
    "        pred = 'nan'\n",
    "    else:\n",
    "        pred = r['<OCR>']\n",
    "    # match only numerical strings\n",
    "    pred = re.sub('[^0-9]', '', pred)\n",
    "    if pred == 'unanswerable' or pred == '':\n",
    "        pred = 'nan'\n",
    "    predictions.append(pred)\n",
    "    \n",
    "df = pd.DataFrame({'ground_truth_label': ground_truth_labels, 'prediction': predictions})\n",
    "correct = df['ground_truth_label'] == df['prediction']\n",
    "df['correct'] = correct\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26 / 36\n",
      "26 / 51\n"
     ]
    }
   ],
   "source": [
    "tp, fp = [], []\n",
    "tn, fn = [], []\n",
    "for row_dict in df.itertuples():\n",
    "    row = list(row_dict)\n",
    "    if row[-1] == True:\n",
    "        tp.append(1)\n",
    "    elif row[1] == 'na' and row[2] == 'nan':\n",
    "        tn.append(1)\n",
    "    elif row[1] == 'na' and row[2] != 'nan':\n",
    "        fp.append(1)\n",
    "    else:\n",
    "        fn.append(1)\n",
    "        \n",
    "precision = sum(tp) / (sum(tp) + sum(fp))\n",
    "recall = sum(tp) / (sum(tp) + sum(fn))\n",
    "\n",
    "print(sum(tp), '/', sum(tp) + sum(fp))\n",
    "print(sum(tp), '/', sum(tp) + sum(fn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7222222222222222"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "26/36"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# precision = tp / (tp + fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Race Identification Benchmark**\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from tqdm import tqdm\n",
    "from glob import glob\n",
    "\n",
    "test_set_dir = '/mnt/opr/levlevi/player-re-id/src/testing/ocr_model_comparions/nba_100_test_set'\n",
    "image_file_paths = glob(os.path.join(test_set_dir, '*.jpg'))\n",
    "PROMPT = \"\"\"Analyze the basketball player shown in the provided still tracklet frame and describe the following details:\n",
    "1. Player Race: Identify the players skin color as either 'black', 'white', 'mixed', or 'asian'.\n",
    "Based on the frame description, produce an output prediction in the following JSON format:\n",
    "{\n",
    "  \"race\": \"<predicted_race>\",\n",
    "}\n",
    "[EOS]\"\"\"\n",
    "\n",
    "def perform_ocr(image_file_path: str) -> str:\n",
    "    image = Image.open(image_file_path)\n",
    "    inputs = processor(text=PROMPT, images=image, return_tensors=\"pt\").to('cuda')\n",
    "    generated_ids = model.generate(\n",
    "        input_ids=inputs[\"input_ids\"],\n",
    "        pixel_values=inputs[\"pixel_values\"],\n",
    "        max_new_tokens=1024,\n",
    "        do_sample=False,\n",
    "        num_beams=3\n",
    "    )\n",
    "    generated_text = processor.batch_decode(generated_ids, skip_special_tokens=False)[0]\n",
    "    parsed_answer = processor.post_process_generation(generated_text, task=\"<CAPTION_TO_PHRASE_GROUNDING>\", image_size=(image.width, image.height))\n",
    "    return parsed_answer\n",
    "\n",
    "ground_truth_labels = []\n",
    "results = []\n",
    "for image_file_path in tqdm(image_file_paths):\n",
    "    # get human annotation\n",
    "    ground_truth_label = image_file_path.split('/')[-1].split('_')[1].split('.')[0]\n",
    "    ground_truth_labels.append(ground_truth_label)\n",
    "    # perform ocr\n",
    "    result = perform_ocr(image_file_path)\n",
    "    results.append(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cog",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
