{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q: how can we represent players as feature vectors?\n",
    "import pandas as pd\n",
    "import os\n",
    "import shutil\n",
    "import json\n",
    "from glob import glob\n",
    "\n",
    "raw_features_fp = '/playpen-storage/levlevi/player-re-id/src/data/raw_features.json'\n",
    "raw_features_df_fp = '/playpen-storage/levlevi/player-re-id/src/data/team_rosters_df.csv'\n",
    "with open(raw_features_fp, 'r') as f:\n",
    "    raw_features = json.load(f)\n",
    "raw_features_df = pd.read_csv(raw_features_df_fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "# which features are redundant?\n",
    "df_redundant_features_dropped = raw_features_df[['team_id', 'player_id', 'team_colors', 'position', 'jersey_number', 'race']]\n",
    "# which features can be one-hot encoded?\n",
    "df_one_hot_team_id = pd.get_dummies(df_redundant_features_dropped, columns=['team_id', 'race',])\n",
    "# segment positions (i.e. multi-category)\n",
    "positions_segmented = []\n",
    "for pos in df_one_hot_team_id['position']:\n",
    "    if len(pos) == 2:\n",
    "        positions_segmented.append([pos])\n",
    "    else:\n",
    "        positions_segmented.append(pos.split('/'))\n",
    "df_positions_segmented = df_one_hot_team_id.copy()         \n",
    "df_positions_segmented['position'] = positions_segmented"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how do we encode positions\n",
    "unique_positions = set(pos for sublist in df_positions_segmented['position'] for pos in sublist)\n",
    "# create columns for each unique position\n",
    "for pos in unique_positions:\n",
    "    df_positions_segmented[f'position_{pos}'] = df_positions_segmented['position'].apply(lambda x: 1 if pos in x else 0)\n",
    "# drop the original position column\n",
    "df_positions_segmented.drop('position', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop team colors for now\n",
    "df_no_team_colors = df_positions_segmented.copy()\n",
    "df_no_team_colors.drop('team_colors', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert Ground-Truth Features to Embeddings (1x45)\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from gensim.models import FastText\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "df = df_no_team_colors.copy()\n",
    "# prepare data for FastText embedding\n",
    "jersey_numbers = df['jersey_number'].astype(str).tolist()\n",
    "sentences = [[char for char in number] for number in jersey_numbers]\n",
    "# train a FastText model\n",
    "model = FastText(sentences, vector_size=30, window=3, min_count=1, sg=1)\n",
    "# get embeddings for jersey numbers\n",
    "def get_embedding(number):\n",
    "    return model.wv[number]\n",
    "df['jersey_embedding'] = df['jersey_number'].apply(lambda x: get_embedding(x))\n",
    "# expand the embeddings into separate columns\n",
    "embeddings = pd.DataFrame(df['jersey_embedding'].tolist(), index=df.index)\n",
    "df = df.drop(columns=['jersey_embedding', 'jersey_number']).join(embeddings)\n",
    "# boolean columns\n",
    "bool_columns = df.select_dtypes(include='bool').columns\n",
    "df[bool_columns] = df[bool_columns].astype(int)\n",
    "# convert all columns to float\n",
    "df = df.astype(float)\n",
    "# feature columns\n",
    "feature_columns = [col for col in df.columns if col != 'player_id']\n",
    "X = df[feature_columns]\n",
    "num_cols = len(X.columns)\n",
    "# remove column names\n",
    "X_no_columns = X.copy()\n",
    "X_no_columns.columns = np.arange(num_cols)\n",
    "# scale all features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X_no_columns)\n",
    "# target column (labels)\n",
    "y = df['player_id']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert Predictions to Embeddings\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how can we convert model predictions to feature vectors?\n",
    "# human labels for 50 tracklet test set\n",
    "with open('/playpen-storage/levlevi/player-re-id/src/testing/ocr_analysis/_50_game_reid_benchmark_/annotations.json') as f:\n",
    "    annotations = json.load(f)\n",
    "# raw mini-cpm predictions\n",
    "with open('/playpen-storage/levlevi/player-re-id/src/testing/ocr_analysis/predictions.json') as f:\n",
    "    predictions = json.load(f)\n",
    "# roster metadata\n",
    "with open(\"/playpen-storage/levlevi/player-re-id/src/data/raw_features.json\", 'r') as f:\n",
    "    rosters = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from typing import List\n",
    "from collections import Counter\n",
    "\n",
    "# get most common race from list\n",
    "def get_maj_race(races):\n",
    "    if len(races) == 0:\n",
    "        return None\n",
    "    counter = Counter(races)\n",
    "    return counter.most_common(1)[0][0]\n",
    "# get most common player postion from list\n",
    "def get_maj_position(positions):\n",
    "    if len(positions) == 0:\n",
    "        return None\n",
    "    counter = Counter(positions)\n",
    "    return counter.most_common(1)[0][0]\n",
    "# get most common jersey number from list\n",
    "def get_maj_jersey_number(jersey_numbers):\n",
    "    if len(jersey_numbers) == 0:\n",
    "        return None\n",
    "    counter = Counter(jersey_numbers)\n",
    "    return counter.most_common(1)[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dataframe for all tracklet predictions\n",
    "predictions_df = pd.DataFrame()\n",
    "# all rows in predictions df\n",
    "rows = []\n",
    "for tracklet_fp, raw_predictions in predictions.items():\n",
    "    # get team ids from file path\n",
    "    team_one_name = tracklet_fp.split('/')[-2].split('_')[3].replace(\" \", \"_\")\n",
    "    team_two_name = tracklet_fp.split('/')[-2].split('_')[5].replace(\" \", \"_\")\n",
    "    team_one_id = rosters[team_one_name]['team_id']\n",
    "    team_two_id = rosters[team_two_name]['team_id']\n",
    "    predicted_races = []\n",
    "    predicted_positions = []\n",
    "    predicted_jersey_numbers = []\n",
    "    potential_team_ids = [team_one_id, team_two_id]\n",
    "    for pred in raw_predictions:\n",
    "        temp_race = pred.get('race')\n",
    "        temp_position = pred.get('position')\n",
    "        temp_jersey_number = pred.get('jersey_number')\n",
    "        if temp_jersey_number:\n",
    "            predicted_jersey_numbers.append(temp_jersey_number)\n",
    "        if temp_race:\n",
    "            predicted_races.append(temp_race)\n",
    "        if temp_position:\n",
    "            predicted_positions.append(temp_position)\n",
    "    maj_race = get_maj_race(predicted_races)\n",
    "    maj_pos = get_maj_position(predicted_positions)\n",
    "    maj_jersey_number = get_maj_jersey_number(predicted_jersey_numbers)\n",
    "    temp_row = [tracklet_fp, maj_race, maj_pos, maj_jersey_number, potential_team_ids]\n",
    "    rows.append(temp_row)\n",
    "# add all rows to df\n",
    "predictions_df = pd.DataFrame(rows, columns=['tracklet_file_path', 'race', 'position', 'jersey_number', 'potential_team_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import sys\n",
    "\n",
    "TEAM_COL_START_IDX = 0\n",
    "TEAM_COL_END_IDX = 30\n",
    "\n",
    "RACE_START_IDX = 30\n",
    "RACE_END_IDX = 34\n",
    "\n",
    "POSITION_START_IDX = 34\n",
    "POSITION_END_IDX = 41\n",
    "\n",
    "JERSEY_NUM_START_IDX = 41\n",
    "\n",
    "# all prediction embeddings\n",
    "all_prediction_embeddings = []\n",
    "# team_ids_column_names\n",
    "team_ids_column_names = list(X.columns)[TEAM_COL_START_IDX:TEAM_COL_END_IDX]\n",
    "# race_column_names\n",
    "race_column_names = list(X.columns)[RACE_START_IDX:RACE_END_IDX]\n",
    "# position_column_names\n",
    "position_column_names = list(X.columns)[POSITION_START_IDX:POSITION_END_IDX]\n",
    "# jersey_column_names\n",
    "jersey_column_names = list(X.columns)[JERSEY_NUM_START_IDX: ]\n",
    "# tracklet file paths\n",
    "tracklet_file_paths = list(predictions_df['tracklet_file_path'])\n",
    "for idx, player_features in predictions_df.iterrows():\n",
    "    # blank feature vector\n",
    "    blank_feature = np.zeros(len(X.columns))\n",
    "    # feature vector index place holder\n",
    "    index = 0\n",
    "    # set one-hot encoded team ids indices to 1\n",
    "    for row_ix, team_str in enumerate(team_ids_column_names):\n",
    "        team_id = int(team_str.split('_')[-1])\n",
    "        potential_team_ids = set(player_features['potential_team_ids'])\n",
    "        if team_id in potential_team_ids:\n",
    "            blank_feature[row_ix] = 1\n",
    "        index += 1\n",
    "    # get player race\n",
    "    predicted_race = player_features['race']\n",
    "    for row_ix, race_str in enumerate(race_column_names):\n",
    "        race_column_label = race_str.split('_')[-1]\n",
    "        if race_column_label == predicted_race:\n",
    "            blank_feature[row_ix + index] = 1\n",
    "        index += 1\n",
    "    # get player position\n",
    "    predicted_position = player_features['position']\n",
    "    if not predicted_position:\n",
    "        predicted_position = ''\n",
    "    for row_ix, position_str in enumerate(position_column_names):\n",
    "        position_column_label = position_str.split('_')[-1]\n",
    "        if position_column_label in predicted_position:\n",
    "            blank_feature[row_ix + index] = 1\n",
    "        index += 1\n",
    "    # get player jersey number\n",
    "    predicted_jersey_number = player_features['jersey_number']\n",
    "    if not predicted_jersey_number:\n",
    "        predicted_jersey_number = -sys.maxsize + 1\n",
    "    # generate embedding\n",
    "    jersey_number_encoding = get_embedding(predicted_jersey_number)\n",
    "    for row_ix, val in enumerate(jersey_number_encoding):\n",
    "        blank_feature[row_ix + index] = val\n",
    "    # add feature vector to list\n",
    "    all_prediction_embeddings.append(blank_feature)\n",
    "\n",
    "# convert to array\n",
    "all_prediction_embeddings = np.array(all_prediction_embeddings)\n",
    "# scale features\n",
    "scaler = StandardScaler()\n",
    "all_prediction_embeddings_scaled = scaler.fit_transform(all_prediction_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "# euclidean Distance\n",
    "def euclidean_distance(vec1: List[float], vec2: List[float]) -> float:\n",
    "    return math.sqrt(sum((x - y) ** 2 for x, y in zip(vec1, vec2)))\n",
    "\n",
    "# cosine Similarity\n",
    "def cosine_similarity(vec1: List[float], vec2: List[float]) -> float:\n",
    "    dot_product = sum(x * y for x, y in zip(vec1, vec2))\n",
    "    magnitude1 = math.sqrt(sum(x ** 2 for x in vec1))\n",
    "    magnitude2 = math.sqrt(sum(y ** 2 for y in vec2))\n",
    "    if magnitude1 == 0 or magnitude2 == 0:\n",
    "        return 0.0  # Avoid division by zero\n",
    "    return dot_product / (magnitude1 * magnitude2)\n",
    "\n",
    "def pearson_correlation(vec1: List[float], vec2: List[float]) -> float:\n",
    "    n = len(vec1)\n",
    "    sum1 = sum(vec1)\n",
    "    sum2 = sum(vec2)\n",
    "    sum1_sq = sum(x ** 2 for x in vec1)\n",
    "    sum2_sq = sum(y ** 2 for y in vec2)\n",
    "    product_sum = sum(x * y for x, y in zip(vec1, vec2))\n",
    "    \n",
    "    numerator = product_sum - (sum1 * sum2 / n)\n",
    "    denominator = math.sqrt((sum1_sq - sum1 ** 2 / n) * (sum2_sq - sum2 ** 2 / n))\n",
    "    if denominator == 0:\n",
    "        return 0.0  # Avoid division by zero\n",
    "    return numerator / denominator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ONLY USE JERSEY NUM EMBEDDINGS\n",
    "# all_prediction_embeddings_scaled = scaler.fit_transform([x[JERSEY_NUM_START_IDX: ] for x in all_prediction_embeddings_scaled])\n",
    "# X_scaled = scaler.fit_transform([x[JERSEY_NUM_START_IDX: ] for x in X_scaled])\n",
    "\n",
    "player_ids_gt = y.copy()\n",
    "human_labels = []\n",
    "\n",
    "for fp in tracklet_file_paths:\n",
    "    video_name = fp.split('/')[-2]\n",
    "    subtrack = fp.split('/')[-1]\n",
    "    human_label = annotations[video_name]['tracks'][subtrack]['human_annotation']\n",
    "    human_labels.append(human_label)\n",
    "    \n",
    "predictions_from_features = [np.nan] * len(human_labels)\n",
    "for pred_fv_idx, feature_vector in enumerate(all_prediction_embeddings_scaled):\n",
    "    best_score = sys.maxsize - 1\n",
    "    best_match_idx = np.nan\n",
    "    for gt_fv_idx, gt_feature_vector in enumerate(X_scaled):\n",
    "        # OPTIONAL: truncate embeddings\n",
    "        euclidean_dist = euclidean_distance(feature_vector, gt_feature_vector)\n",
    "        if euclidean_dist < best_score:\n",
    "            best_score = euclidean_dist\n",
    "            best_match_idx = gt_fv_idx\n",
    "            \n",
    "    predictions_from_features[pred_fv_idx] = player_ids_gt[best_match_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a dataframe with all predictions\n",
    "predictions_computed = pd.DataFrame({'tracklet_file_path': tracklet_file_paths, 'human_label': human_labels, 'prediction': predictions_from_features})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find all matching predictions\n",
    "matched_mask = predictions_computed['human_label'] == predictions_computed['prediction']\n",
    "predictions_computed_matched = predictions_computed[matched_mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert all ids to floats\n",
    "predictions_computed_no_na = predictions_computed_matched.dropna()\n",
    "predictions_computed_no_na['human_label'] = predictions_computed_no_na['human_label'].astype(int)\n",
    "predictions_computed_no_na['prediction'] = predictions_computed_no_na['prediction'].astype(int)\n",
    "# find all matching predictions\n",
    "matched_mask = predictions_computed_no_na['human_label'] == predictions_computed_no_na['prediction']\n",
    "predictions_no_na_matched = predictions_computed_no_na[matched_mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions_no_na_matched.__len__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vish",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
