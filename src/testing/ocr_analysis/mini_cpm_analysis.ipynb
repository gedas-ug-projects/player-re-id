{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from PIL import Image\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "\n",
    "model = AutoModel.from_pretrained('openbmb/MiniCPM-Llama3-V-2_5', trust_remote_code=True, torch_dtype=torch.float16)\n",
    "model = model.to(device='cuda')\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('openbmb/MiniCPM-Llama3-V-2_5', trust_remote_code=True)\n",
    "model.eval()\n",
    "\n",
    "fp = '/playpen-storage/levlevi/player-re-id/src/testing/ocr_analysis/sample_tracks_nba_100/__jersey_number_ocr_benchmark_nba_100__/1_16.jpg'\n",
    "image = Image.open('fp').convert('RGB')\n",
    "question = 'What is in the image?'\n",
    "msgs = [{'role': 'user', 'content': question}]\n",
    "\n",
    "res = model.chat(\n",
    "    image=image,\n",
    "    msgs=msgs,\n",
    "    tokenizer=tokenizer,\n",
    "    sampling=True, # if sampling=False, beam_search will be used by default\n",
    "    temperature=0.7,\n",
    "    # system_prompt='' # pass system_prompt if needed\n",
    ")\n",
    "print(res)\n",
    "\n",
    "## if you want to use streaming, please make sure sampling=True and stream=True\n",
    "## the model.chat will return a generator\n",
    "res = model.chat(\n",
    "    image=image,\n",
    "    msgs=msgs,\n",
    "    tokenizer=tokenizer,\n",
    "    sampling=True,\n",
    "    temperature=0.7,\n",
    "    stream=True\n",
    ")\n",
    "\n",
    "generated_text = \"\"\n",
    "for new_text in res:\n",
    "    generated_text += new_text\n",
    "    print(new_text, flush=True, end='')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mini",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.1.-1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
